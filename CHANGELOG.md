# 0.0.10

- Preload the model heads so full model can be quantized.
- Implement CPU offloading of optional network components.
- Add max concurrency for model inference.
- Additional generation settings, schedule, strategy, annealing, etc.

# 0.0.9

- Removed group quantization.
- Added model info endpoint.

# 0.0.8

- Added quantization group size setting.
- Function predictions now have start and end annotations.
